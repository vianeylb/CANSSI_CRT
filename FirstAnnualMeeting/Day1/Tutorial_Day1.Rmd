---
title: "HMMs: Generation of hidden and observable process, forward algorithm, and computation of the likelihood"
author: "Marco Gallegos Herrada"
date: "8/11/2022"
output: 
  bookdown::html_document2:
    number_sections: true
    highlight: tango
editor_options:
  chunk_output_type: console
---

<!-- To be able to have continuous line numbers -->
```{=html}
<style>
body
  { counter-reset: source-line 0; }
pre.numberSource code
  { counter-reset: none; }
</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(message = FALSE)
```



# Tutorial goals

The goal of this tutorial is to implement the theory behind hidden Markov models and see how to generate a sequence of hidden states as well as simulate observations from the corresponding state-dependen distributions. More precisely, we will generate observations from a 2-state HMM where the state-dependent distributions correspond to Poisson distributions. Throughout this tutorial, we will work with an homogeneous and stationary Markov process for the hidden process.

The primary learning objectives in this first tutorial are to:

  1. Generate a sequence of hidden states given an initial distribution $\pi$, t.p.m. $\Gamma$ and vector of parameters $\Lambda$ (i.e. $\eta = (\pi,\Gamma,\Lambda)$),
  2. Simulate a sequence of observations from the state-dependent distributions given a sequence of hidden states,
  3. Explore how to visualize both hidden and observable process,
  4. Implementation of the forward algorithm for the computation of the probabilities
  
  \begin{align*}
  \alpha_{j,t} &= P(Y_{1:t}=y_{1:t} \mid X_t = j),\\ 
  \end{align*} 
  
  5. The computation of the likelihood evaluated in $\eta$, $\mathcal{L}(\eta)$.

# Model

Let $\pi = (\pi_1,  \pi_2)$ the initial distribution of our Markov chain, such that 

\begin{align*}
  \pi_1 = P(X_1 = 1) = 2/3\\
  \pi_2 = P(X_1 = 2) = 1/3
\end{align*}

As well, let's consider the transition probability matrix with the following values

\begin{align*}
\Gamma = 
\begin{bmatrix}
0.9 & 0.1\\
0.2 & 0.8
\end{bmatrix}
\end{align*}

We will assume

\begin{align*}
f(y_t \mid X_t = 1) &\sim Poisson(\lambda_1 = 3),\\
f(y_t \mid X_t = 2) &\sim Poisson(\lambda_2 = 10).
\end{align*}


# Data generation

  **a)** Simulate 100 states from a 2-state Markov chain with initial distribution and t.p.m. as provided above. For the time $t=1$ (initial time), use `set.seed(10)` and for $t=2,\ldots,100$, use `set.seed(20)`. Store your state sequence in a variable called `state_seq`.

  **b)** Visualize the generated state sequence.

  **c)** Simulate 100 observations from the state-dependent distributions given the hidden states that you just generated. Set `set.seed(30)` to generate such observations and store them in a vector called `obs_seq`.

  **d)** Visualize the data generated from the state-dependent distributions, along with the parameters values $\lambda_1, \lambda_2$.

## Let's get to do some code! (First part)

**1a)** A 2-state Markov chain with pi and t.p.m. provided previously

```{r}

# Initial distribution
pi = c(2/3,1/3)

# Transition probability matrix
Gamma.matrix = matrix(c(.9,.2,.1,.8),2,2)

```

```{r}
# Number of times to consider
ntimes <- 100
state_seq <- c()

set.seed(10)
# The function sample() could be useful to simulate the initial distribution!
state_seq[1] <- sample(1:2, 1, prob = pi)

########################################
### Generate the state sequence here ###
########################################
set.seed(20)

for (i in 2:ntimes) {
  state_seq <- c(state_seq, sample(1:2, 1, prob = Gamma.matrix[state_seq[i-1], ]))
}

state_seq

```

**1b)** Visualize the state sequence

```{r}

############################################################################
### Include your code for visualizing your sequence of observations here ###
############################################################################

plot(x = 1:ntimes, y = state_seq, type = "s", main = "State sequence", xlab = "Time", ylab = "state")

```


**1c)** Simulate 100 observation from the state-dependent distributions given the hidden states that you just generated. 

```{r}

# Vector of parameters corresponding to the state-dependent distributions
lambda = c(3,10)

###################################################
### Generate your sequence of observations here ###
###################################################
set.seed(30)
obs_seq <- c()

for (i in 1:ntimes) {
  obs_seq <- c(obs_seq, rpois(1, lambda[state_seq[i]]))
}

obs_seq

``` 

**1d)** Visualize the data generated from the state dependent distributions, along with the lambda values

```{r}

############################################################################
### Include your code for visualizing your sequence of observations here ###
############################################################################

plot(obs_seq,type="l")
abline(h=3)
abline(h=10)

```


# The forward algorithm. Computation of $\mathcal{L}(\eta)$.

**a)** Let's recall that the forward variables $\alpha_t$ are defined as

\begin{align*}
    \boldsymbol{\alpha}_1 &= \boldsymbol{\pi}\mathbf{P}(y_1),\\
\end{align*}
    and
\begin{align*}
    \boldsymbol{\alpha}_t = \boldsymbol{\pi}\mathbf{P}(y_1)\boldsymbol{\Gamma}\mathbf{P}(y_2) \cdots \boldsymbol{\Gamma}\mathbf{P}(y_t) &= \boldsymbol{\alpha}_{t-1}\Gamma P(y_t) \quad  \text{for $t>1$}. 
\end{align*}

Create a function that computes the forward algorithm $\alpha_t$ for any time $t = 1,...,T$.



**b)** Since the likelihood is a product of matrices, not of scalars, it is not possible to circumvent numerical underflow simply by computing the log of the likelihood as the sum of logs of its factors. To overcome this issue, we can compute the logarithm of $\mathcal{\eta}$ by using a strategy of scaling the vector of forward probabilities $\alpha_t$. Effectively we scale the
vector $\alpha_t$ at each time $t$ so that its elements add to 1, keeping track of
the sum of the logs of the scale factors thus applied. 

More formally, for each $t=0,\ldots,T$, let's define the vector

\begin{align*}
  \phi_t = \alpha_t/w_t
\end{align*}

where $w_t = \sum_i = \alpha_{i,t} = \alpha_t 1'$, and define $w_0 = 1$, $\alpha_0 = \pi$. It follows from the definition of $\phi_t$ and $w_t$ the following properties:

\begin{align*}
  w_0 = \alpha_0 1' &= \pi 1' = 1\\
  \phi_0 &= \pi\\
  w_t\phi_t &= w_{t-1}\phi_{t-1} \Gamma P(y_t)\\
  \mathcal{L}(\eta) = \alpha_T 1' &= w_T (\phi_T 1') = w_T.
\end{align*}

Hence $\displaystyle L_T = w_T = \prod_{i=1} ^{T} (w_t/w_{t-1})$. From the third equation above, it follows that

\begin{align*}
  w_t = w_{t-1} (\phi_{t-1} \Gamma P(y_t) 1'),
\end{align*}

and so we conclude

\begin{align*}
  \displaystyle  l(\eta) = log(\mathcal{L}(\eta)) = \sum_{i=1}^{T} log(w_t / w_{t-1}) = \sum_{t=1}^{T} log((\phi_{t-1} \Gamma P(y_t) 1')).
\end{align*}

Compute the log-likelihood using the scale vector strategy.


## Let's get to do some code! (Second part)

**2a)** Create a function that computes the forward algorithm for any time $t = 1,...,T$.

```{r}

################################################################################
### Include your function that computes the forward algorithm for any t here ###
################################################################################

# Function that computes the forward algorithm for any t

forward_algorithm = function(observations,time,init = pi,tpm=Gamma.matrix,parameters=lambda){
  
  # This is for time t = 1
  alpha = init %*% diag(dpois(observations[1],parameters))
  

  #Alternative way of computing alpha for t = 1
  #alpha = init * dpois(observations[1],parameters)
  
  if(time > 1){
    for (i in 2:time) {
      
      alpha = alpha %*% tpm%*%diag(dpois(observations[i],parameters))
        
      #Alternative way of computing alpha for t > 1
      #alpha = alpha %*% tpm*dpois(observations[i],parameters)
    }
  }
  return(alpha)
}


```


**2b)** Compute the log-likelihood using the scale vector strategy.

```{r}

############################################################################################
### Include your fct that computes the log-likehood using the scale vector strategy here ###
############################################################################################

### Function that computes the forward algorithm in the logarithm scale for any t

log_forward_algorithm = function(observations,time,init = pi,tpm=Gamma.matrix,parameters=lambda){
  
  alpha = init*dpois(observations[1],lambda)
  
  # log(alpha) for time t = 1
  lscale <- log(sum(alpha))
  alpha <- alpha/sum(alpha)
  
  # For time t > 1, we have that:
  for (i in 2:time) {
    alpha <- alpha %*% tpm%*%diag(dpois(observations[i],lambda))
    
    #Alternative way of computing alpha for t > 1
    #alpha <- alpha %*% tpm*dpois(observations[i],lambda)
    
    lscale <- lscale+log(sum(alpha))
    alpha <- alpha/sum(alpha)
  }
  
  return(lscale)

}

log_forward_algorithm(obs_seq,ntimes)


```

