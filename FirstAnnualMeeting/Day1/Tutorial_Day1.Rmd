---
title: "Hidden Markov Models: Generation of hidden and observable process, forward and backward algorithm, and computation of likelihood"
author: "Marco Gallegos Herrada"
date: "8/11/2022"
output: 
  bookdown::html_document2:
    number_sections: true
    highlight: tango
editor_options:
  chunk_output_type: console
---

<!-- To be able to have continuous line numbers -->
```{=html}
<style>
body
  { counter-reset: source-line 0; }
pre.numberSource code
  { counter-reset: none; }
</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(message = FALSE)
```



# Tutorial goals

The goal of this tutorial is to implement the theory behind hidden Markov models and see how to generate a sequence of hidden states as well as simulate observations from the corresponding state-dependen distributions. More precisely, we will generate observations from a 2-state HMM where the state-dependent distributions correspond to Poisson distributions. Throughout this tutorial, we will work with an homogeneous and stationary Markov process for the hidden process.

The primary learning objectives in this first tutorial are to:

  1. Generate a sequence of hidden states given an initial distribution and t.p.m.,
  2. Simulate a sequence of observations from the state-dependent distributions given a sequence of hidden states,
  3. Explore how to visualize both hidden and observable process,
  4. Implementation of the forward and backward algorithms for the computation of the probabilities
  
  \begin{align*}
  \alpha_t(i) &= P(X_1=x_1,\ldots,X_t=x_t\mid C_t = i)\\ 
  \beta_t(i) &= P(X_1=x_1,\ldots,X_t = x_t, C_t = i),
  \end{align*} 
  
  5. The computation of the likelihood $L_T$.

# Model

Let $\delta = (\delta_1,  \delta_2)$ the initial distribution of our Markov chain, such that 

\begin{align*}
  \delta_1 = P(C_1 = 1) = 2/3\\
  \delta_2 = P(C_1 = 2) = 1/3
\end{align*}

As well, let's consider the transition probability matrix with the following values

\begin{align*}
\Gamma = 
\begin{bmatrix}
0.9 & 0.1\\
0.2 & 0.8
\end{bmatrix}
\end{align*}

We will assume

\begin{align*}
f(x_t \mid C_t = 1) &\sim Poisson(3),\\
f(x_t \mid C_t = 2) &\sim Poisson(10).
\end{align*}


# Data generation

  **a)** Simulate 100 states from a 2-state Markov chain with initial distribution and t.p.m. as provided above. For the time $t=1$ (initial time), use `set.seed(10)` and for $t=2,\ldots,100$, use `set.seed(20)`. Store your state sequence in a variable called `state_seq`.

  **b)** Visualize the generated state sequence.

  **c)** Simulate 100 observations from the state-dependent distributions given the hidden states that you just generated. Set `set.seed(30)` to generate such observations and store them in a vector called `obs_seq`.

  **d)** Visualize the data generated from the state-dependent distributions, along with the lambda values.

## Let's get to do some code! (First part)

**1a)** A 2-state Markov chain with delta and t.p.m. provided previously

```{r}

# Initial distribution
#delta = 

# Transition probability matrix
#Gamma.matrix = 

```

```{r}
# Number of times to consider
ntimes <- 100
#state_seq <- 

set.seed(10)
# The function sample() could be useful to simulate the initial distribution!
#state_seq[1] <- 

########################################
### Generate the state sequence here ###
########################################
set.seed(20)

```

**1b)** Visualize the state sequence

```{r}

############################################################################
### Include your code for visualizing your sequence of observations here ###
############################################################################


```


**1c)** Simulate 100 observation from the state-dependent distributions given the hidden states that you just generated. 

```{r}

# Vector of parameters corresponding to the state-dependent distributions
lambda = c(3,10)

###################################################
### Generate your sequence of observations here ###
###################################################
set.seed(30)
#obs_seq <-

``` 

**1d)** Visualize the data generated from the state dependent distributions, along with the lambda values

```{r}

############################################################################
### Include your code for visualizing your sequence of observations here ###
############################################################################

```


# The forward and backward algorithms. Computation of $L_T$.

**a)** Create a function that computes the forward algorithm $\alpha_t$ for any time $t = 1,...,T$.

**b)** Create a function that computes the backward algorithm $\beta'_t$ for any time $t = 1,...,T$.

**c)** Since the likelihood is a product of matrices, not of scalars, it is not possible to circumvent numerical underflow simply by computing the log of the likelihood as the sum of logs of its factors. To overcome this issue, we can compute the logarithm of $L_T$ by using a strategy of scaling the vector of forward probabilities $\alpha_t$. Effectively we scale the
vector $\alpha_t$ at each time $t$ so that its elements add to 1, keeping track of
the sum of the logs of the scale factors thus applied. 

More formally, for each $t=1,\ldots,T$, let's define the vector

\begin{align*}
  \phi_t = \alpha_t/w_t
\end{align*}

where $w_t = \sum_i = \alpha_t(i) = \alpha_t 1'$. It follows from the definition of $\phi_t$ and $w_t$ the following properties:

\begin{align*}
  w_1 = \alpha_0 1' &= \delta 1' = 1\\
  \phi_1 &= \delta\\
  w_t\phi_t &= w_{t-1}\phi_{t-1} \Gamma P(x_t)\\
  L_T = \alpha_T 1' &= w_T (\phi_T 1') = w_T.
\end{align*}

Hence $\displaystyle L_T = w_T = \prod_{i=1} ^{T} (w_t/w_{t-1})$. From the third equation above, it follows that

\begin{align*}
  w_t = w_{t-1} (\phi_{t-1} \Gamma P(x_t) 1'),
\end{align*}

and so we conclude

\begin{align*}
  \displaystyle  log(L_T) = \sum_{i=1}^{T} log(w_t / w_{t-1}) = \sum_{t=1}^{T} log((\phi_{t-1} \Gamma P(x_t) 1')).
\end{align*}

Compute the log-likelihood using the scale vector strategy.

**d)** Show that, for $t=43$, in effect $\alpha_{43}\beta'_{43} = L_T$.

## Let's get to do some code! (Second part)

**2a)** Create a function that computes the forward algorithm for any time $t = 1,...,T$.

```{r}

################################################################################
### Include your function that computes the forward algorithm for any t here ###
################################################################################

```

**2b)** Create a function that computes the backward algorithm $\beta'_t$ for any time $t = 1,...,T$.

```{r}

#######################################################################
### Include your function that computes the backward algorithm here ###
#######################################################################

### Tip: Compute (beta_t)' it's easier than computing beta_t!

``` 

**2c)** Compute the log-likelihood using the scale vector strategy.

```{r}

############################################################################################
### Include your fct that computes the log-likehood using the scale vector strategy here ###
############################################################################################

```

**2d)** Show that, for $t=43$, in effect $\alpha_{43}\beta'_{43} = L_T$.

```{r}

##############################
### Include your code here ###
##############################

```