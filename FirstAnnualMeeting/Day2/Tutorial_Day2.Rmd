---
title: "Hidden Markov Models: Direct maximization, dealing with missing data in the likelihood and state decoding"
author: "Marco Gallegos Herrada"
date: "8/11/2022"
output: 
  bookdown::html_document2:
    number_sections: true
    highlight: tango
editor_options:
  chunk_output_type: console
---

<!-- To be able to have continuous line numbers -->
```{=html}
<style>
body
  { counter-reset: source-line 0; }
pre.numberSource code
  { counter-reset: none; }
</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(message = FALSE)
```


# Tutorial goals

In yesterday's tutorial, we addressed how to simulate and visualize a hidden state sequence and an observable process derived from for a given $\eta = (\pi,\Gamma,\Lambda)$, and the computation of $l(\eta)$ through the forward algorithm. In this tutorial, we will deal with technical issues for maximimizing the likelihood, the computation of the likelihood when there are data missing and the implementation of the forward/backward algorithm for state decoding. More specificly:
  
  
  - Modifying the likelihood when data are missing
    - Missing observations
    - Interval censoring
  - Direct maximization
    - Dealing with parameter constrains
    - Bayesian estimation
  - Implementation of the backward algorithm for the computation of the probabilities
  
  \begin{align*}
    \beta_{j,t} &= P(Y_{1:t}=y_{1:t} \mid X_t = j) 
  \end{align*}
  
  - State decoding
    - Local decoding

For this purpose, we will reuse the sequence of states and observations generated in yesterday's tutorial:

```{r}

gdata = readRDS("gdata.rds")

states_seq = gdata$states
obs_seq = gdata$observations
ntimes = 100

source("aux-fcts.R")

```

# Modyfing the likelihood

## The likelihood when data are missing

It's pausible that, when working with time series data, some observations are missing. However, in the case of HMM, the adjustemnt of the likelihood computation if data are missing turns out to be relatively simple.

Let's consider, for example, that one has available the observations $x_1, x_3,x_6,x_8,x_{9},\ldots, x_{T}$ of an HMM, but $x_2,x_4,x_5,x_7$ are missing. Then the likelihood of the observation is given by

\begin{align*}
  P(Y_1=y_1 &, Y_3=y_3,Y_6=y_6,Y_8=y_8,Y_{9}=y_{9},\ldots, Y_T=y_{T})\\ 
  = &\sum \delta_{x_1}\gamma_{x_1,x_3}(2)\gamma_{x_3,x_6}(3)\gamma_{x_6,x_8}(2)\gamma_{x_8,x_9}\cdots,\gamma_{x_{T-1},x_T}\\
  &\times p_{x_1}(y_1)p_{x_3}(y_3)p_{x_6}(y_6)p_{x_8}(y_8)\cdots p_{x_T}(y_T)
\end{align*}

where $\gamma_{ij}(k)$ denotes a $k-$step transition probability, and the sum is taken over all indices $x_t$ other than $x_2,x_4,x_5,x_7$. But this is in fact 

\begin{align*}
  &= \pi P(y_1)\Gamma^2 P(y_3) \Gamma^{3} P(y_6) \Gamma^{2} P(y_8) \Gamma P(y_9) \cdots \Gamma P(y_T)1'.
\end{align*}

From this, we can conclude that, in general, in the expression for the likelihood the diagonal matrices $P(y_t)$ corresponding to missing observations $y_t$ can be replaced by the identity matrix!

Given the above, now let's consider our original sequence of observations, but now we will assume that there's some missing observations:

```{r}


set.seed(50)
missing_id = sample(1:ntimes,30)

obs_seq_missing = obs_seq

obs_seq_missing[missing_id] = NA

obs_seq_missing

```

Using the approach shown above, compute the likelihood of the sequence with missing data. 

```{r}

#########################################################
### Enter your code for computing the likelihood here ###
#########################################################

```

## The likelihood when observations are interval-censored

Assume that some of the missing observations are interval-censored. For instance, the value of $x_t$ may be known only for $t \neq 1,3,8$, with the information 

\begin{align*}
  y_1 \leq 5,\\
  2 \leq y_3 \leq 3\\
  y_8 > 10,
\end{align*}
  available about the remaining observations. In this scenario one replaces the diagonal matrices $P(y_i)$ $(i = 1,3,8)$ in the likelihood expression by the matrices
  
\begin{align*}
  \text{diag} (P(Y_1 \leq 5 \mid X_1 = 1), P(Y_1 \leq 5 \mid X_1 = 2)),\\
  \text{diag} (P(2 \leq Y_2 \leq 3 \mid X_1 = 1), P(2 \leq Y_2 \leq 3 \leq 5 \mid X_1 = 2)),\\
  \text{diag} (P(Y_8 > 10 \mid X_1 = 1), P(Y_8 > 10 \mid X_1 = 2)).
\end{align*}

Compute the likelihood of the observations given the information above, and the observation sequence is $T=10$. Compare the likelihood of the first 10 observations but assuming that all of these are observed. Which likelihood is bigger and why?

```{r}

#################################################################
### Compute the likelihood when given interval censoring here ###
#################################################################


```


# Direct maximization

```{r}


log.likelihood = function(theta.star,observations=obs_seq){
  
  theta = c(plogis(theta.star[1]),plogis(theta.star[2]),exp(theta.star[3]),exp(theta.star[4]))
  # This is for time t = 1
  tpm = diag(theta[1:2])
  tpm[1,2] = 1-tpm[1,1]
  tpm[2,1] = 1-tpm[2,2]
  
  pi = solve(t(diag(2)-tpm + matrix(1,2,2)),c(1,1))
  lambda = theta[3:4]
  
  alpha = pi %*% diag(dpois(observations[1],lambda))
  
  for (i in 2:length(obs_seq)) {
    #alpha = alpha %*% tpm*dpois(observations[i],parameters)
    
    #Alternative way of computing alpha for t > 1
    alpha = alpha %*% tpm%*%diag(dpois(observations[i],lambda))
  }
  return(-log(sum(alpha))) # returns -log(L) since nlm() can only minimize!
}


set.seed(30)
dgammas = runif(2)
lambdas = runif(1)
lambdas = c(5,12)

theta.star = c(qlogis(dgammas),log(lambdas))

```


```{r,message=FALSE}

mod = nlm(log.likelihood,theta.star,observations=obs_seq)

c(plogis(mod$estimate[1:2]),exp(mod$estimate[3:4]))

```

## Bayesian inference



```{r,message=FALSE}

library(cmdstanr)
model = cmdstan_model("hmm_poisson.stan")
data = list(N=ntimes,K=2,y=obs_seq)
fit = model$sample(data)

```

```{r}

print(fit)

```


# Backward algorithm

**a)** Create a function that computes the backward algorithm $\beta'_t$ for any time $t = 1,...,T$.
**b)** Show that, for $t=43$, in effect $\alpha_{43}\beta'_{43} = L_T$.

**4a)** Create a function that computes the backward algorithm $\beta'_t$ for any time $t = 1,...,T$.

```{r}

#######################################################################
### Include your function that computes the backward algorithm here ###
#######################################################################

### Tip: Compute (beta_t)' it's easier than computing beta_t!

``` 

**4b)** Show that, for $t=43$, in effect $\alpha_{43}\beta'_{43} = L_T$.

```{r}

##############################
### Include your code here ###
##############################

```


# State decoding

## Local Decoding

From the introduction of HMM previously provided, we know that for every $t=1,\ldots , T$

\begin{align*}
  \alpha_{i,t}\beta_{i,t} = P(Y_{1:T}=y_{1:T} , X_t = i).
\end{align*}

And thus, the conditional distribution of $X_t$ given the observations can be obtained as

\begin{align*}
  P(X_t = i \mid Y_{1:T} = y_{1:T}) &= \dfrac{P(X_t = i, Y_{1:T} = y_{1:T})}{P(Y_{1:T} = y_{1:T})}\\
  &= \dfrac{\alpha_t(i)\beta_t(i)}{\mathcal{L}}.
\end{align*}

Use your functions for computing the forward and backward algorithms from previous tutorial to estimate $P(X_t = i \mid Y_{1:T} = y_{1:T})$ for every time $t=1,\ldots, T$ and every state $i=1,2$. Plot the estimated probabilities for the state 1 for every $t$.

```{r}

#################################################
### Include your code for local decoding here ###
#################################################

```

